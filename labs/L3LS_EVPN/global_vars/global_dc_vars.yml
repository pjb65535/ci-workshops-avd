---
# Credentials for CVP (legacy arista.avd.eos_config_deploy_cvp role) and EOS Switches
ansible_user: arista
ansible_password: "{{ lookup('env', 'LABPASSPHRASE') }}"
ansible_network_os: arista.eos.eos
# Configure privilege escalation
ansible_become: true
ansible_become_method: enable
# HTTPAPI configuration
ansible_connection: httpapi
ansible_httpapi_port: 443
ansible_httpapi_use_ssl: true
ansible_httpapi_validate_certs: false
ansible_python_interpreter: $(which python3)
avd_data_validation_mode: error

# arista.avd.cv_deploy role parameters
cv_token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJkaWQiOjc1MDQ0MDYzMzQwNjI3MjMzMTYsImRzbiI6IkFWRCIsImRzdCI6ImFjY291bnQiLCJleHAiOjE3NTY1OTQ4MDgsImlhdCI6MTc1MzI3ODcyNCwib2dpIjozLCJvZ24iOiJEZWZhdWx0Iiwic2lkIjoiYmEyYzk4NjZmZTgyYjA4NGUzY2RjZWQwNGQxM2JlMTUxMGQwYTM0Yzc1MzI1OTJiMWUyOWQ0NjY0MWYyMDUwNC05d0JHM18yMThwUU1nbzFtNzdjQWthUXFIbzc1SEdZdW9QeWJzdTZwIn0.ozPTPa2_AOhUKDzchxwNr3dBrao0gy2KsjzeEaETI-1WX7ImaRCcMi2rXSBeRLsQR9jMSGid8OXcjFFpMvXqeYlkKlN_g5ogHKVR4gTBrmvKZsuQTSj_rukFzWHzQFrJLOFy4ZZOl7lJbuf08dJHJ1TJSChJS34E7vWbhhueh1W-3w_k8F77L5GU-6E_WM1-Bowe2a57u3dMEj-NIWvCqxA66CS1mF_7gX8jIm0_cXJj3U3-EVFoSxKaS2CGnSU3RiM2G0s6QFAKozMuL_2bWSvLOXO2tnKcdAHil94GUNHO-Ag69fUdkYxqc6v_7FsIgGmEdI4NPH7RBxnDgLkyj2ogGVhPaJ6rhUXYhqXfokabz4i4SSHwMkmSScTd6ejfILQgnN4DZmUOYFng6gP4KnsFzxljktDqgfaaxshkRULwbYYdylnyZet2JuEj3AdebmmodDRMG2No2UKmr7W85dLAI9Yw7NhGlGnAV85rNibwKBwEgw6K-3HxG_2CzJC4vftxHWZ9oIxzlBLhNu-RapnufWEVZyHJGHAZzU2RKVyMfhiopTEn3qi_JgDoToxurTAtTD2SRqKaZVXj8bpjVIwFO4rNkEg0KXvPn_70b2tDkqNhdvcd3HPMgcW0YSVdBCLBHYzaZNLWx-cTtaCkr89X5d4-ws41zEUaVZEdCZg
cv_server: cvp
cv_verify_certs: false

# arista.avd.eos_config_deploy_cvp role parameters
cv_collection: v3
execute_tasks: true

# Local Users
local_users:
  - name: arista
    privilege: 15
    role: network-admin
    sha512_password: "{{ ansible_password | password_hash('sha512', salt='arista', rounds=5000) }}"
    ssh_key: "{{ lookup('ansible.builtin.file', '~/.ssh/id_rsa.pub') }}"

# AAA
aaa_authorization:
  exec:
    default: local

# OOB Management network default gateway.
mgmt_gateway: 192.168.0.1
mgmt_interface_vrf: default

# NTP Servers IP or DNS name, first NTP server will be preferred, and sourced from Management VRF
ntp:
  servers:
    - name: 192.168.0.1
      iburst: true
      local_interface: Management0

# Domain/DNS
dns_domain: atd.lab

# TerminAttr
daemon_terminattr:
  # Address of the gRPC server on CloudVision
  # TCP 9910 is used on on-prem
  # TCP 443 is used on CV as a Service
  cvaddrs: # For single cluster
    - 192.168.0.5:9910
  # Authentication scheme used to connect to CloudVision
  cvauth:
    method: token
    token_file: "/tmp/token"
  # Exclude paths from Sysdb on the ingest side
  ingestexclude: /Sysdb/cell/1/agent,/Sysdb/cell/2/agent
  # Exclude paths from the shared memory table
  smashexcludes: ale,flexCounter,hardware,kni,pulse,strata

# Point to Point Links MTU Override for Lab
p2p_uplinks_mtu: 1500

# Set IPv4 Underlay Routing and EVPN Overlay Routing to use eBGP
underlay_routing_protocol: ebgp
overlay_routing_protocol: ebgp

# Configure password authentication for BGP peerings
bgp_peer_groups:
  evpn_overlay_peers:
    password: Q4fqtbqcZ7oQuKfuWtNGRQ==
  ipv4_underlay_peers:
    password: 7x4B4rnJhZB438m9+BrBfQ==
  mlag_ipv4_underlay_peer:
    password: 4b21pAdCvWeAqpcKDFMdWw==

# # L3 Edge port definitions. This can be any port in the entire Fabric, where IP interfaces are defined.
l3_edge:
  # Define a new IP pool that will be used to assign IP addresses to L3 Edge interfaces.
  p2p_links_ip_pools:
    - name: S1_to_S2_IP_pool
      ipv4_pool: 172.16.255.0/24
  # Define a new link profile which will match the IP pool, the used ASNs and include the defined interface into underlay routing
  p2p_links_profiles:
    - name: S1_to_S2_profile
      ip_pool: S1_to_S2_IP_pool
      as: [ 65103, 65203 ]
      include_in_underlay_protocol: true
  # Define each P2P L3 link and link the nodes, the interfaces and the profile used.
  p2p_links:
    - id: 1
      nodes: [ s1-brdr1, s2-brdr1 ]
      interfaces: [ Ethernet4, Ethernet4 ]
      profile: S1_to_S2_profile
    - id: 2
      nodes: [ s1-brdr2, s2-brdr2 ]
      interfaces: [ Ethernet5, Ethernet5 ]
      profile: S1_to_S2_profile


